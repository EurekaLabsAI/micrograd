<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>micrograd</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            overflow: hidden;
        }
        #container {
            width: 100%;
            height: 100%;
        }
    </style>
</head>
<body>
    <div id="container"></div>
    <script src="micrograd.js"></script>

<script>
// Create an instance of RNG with seed 42
const random = new RNG(42);
// Generate data using the genData function
const dataSplits = genData(random, 100);
const trainSplit = dataSplits.train;
const valSplit = dataSplits.validation;
const testSplit = dataSplits.test;
// init the model: 2D inputs, 16 neurons, 3 outputs (logits)
const model = new MLP(2, [16, 3]);
// optimize using AdamW
const optimizer = new AdamW(model.parameters(), 1e-1, [0.9, 0.95], 1e-8, 1e-4);

function train_step() {
    // forward the network and the loss function on all training datapoints
    let loss = new Value(0);
    for (const [x, y] of trainSplit) {
        const logits = model.forward([new Value(x[0]), new Value(x[1])]);
        loss = loss.add(crossEntropy(logits, y));
    }
    loss = loss.mul(1.0 / trainSplit.length); // normalize the loss
    // backward pass (deposit the gradients)
    loss.backward();
    // update with AdamW
    optimizer.step();
    optimizer.zeroGrad();
    return loss.data;
}

for (let step = 0; step < 100; step++) {
    // evaluate the validation split every few steps
    if (step % 10 === 0) {
        const valLoss = evalSplit(model, valSplit);
        console.log(`step ${step}, val loss ${valLoss.toFixed(6)}`);
    }
    // train for one iteration
    const trainLoss = train_step();
    console.log(`step ${step}, train loss ${trainLoss}`);
}

</script>
</body>
</html>
