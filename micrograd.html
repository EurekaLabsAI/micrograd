<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>micrograd</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
        }
        #canvas-div {
            margin: 5px;
        }
        #decision-canvas {
            border: 1px solid black;
        }
        .container {
            display: flex;
            justify-content: flex-start;
            align-items: flex-start;
        }
        #optimizer-div {
            margin: 5px;
            font-size: 14px;
            max-height: 500px;
            overflow-y: auto;
        }
        #optimizer-div table {
            border-collapse: collapse;
        }
        #optimizer-div th, #optimizer-div td {
            border: 1px solid #ddd;
            padding: 4px 8px;
            text-align: right;
        }
        #optimizer-div th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

    <div class="container">
        <!-- The canvas div that displays the current decision boundary of the MLP -->
        <div id="canvas-div">
            <canvas id="decision-canvas" width="500" height="500">Browser not supported for Canvas.</canvas><br /><br />
        </div>
        <!-- The div that displays the AdamW optimizer state: param, grad, m, v -->
        <div id="optimizer-div">
        </div>
    </div>

    <!-- The micrograd JavaScript lib -->
    <script src="micrograd.js"></script>

<script>
// Create an instance of RNG with seed 42
const random = new RNG(42);
// Generate data using the genData function
const dataSplits = genDataYinYang(random, 100);
const trainSplit = dataSplits.train;
const valSplit = dataSplits.validation;
const testSplit = dataSplits.test;
// init the model: 2D inputs, 8 neurons, 3 outputs (logits)
const model = new MLP(2, [8, 3]);
// optimize using AdamW
const optimizer = new AdamW(model.parameters(), 1e-1, [0.9, 0.95], 1e-8, 1e-4);

function lossFun(model, split) {
    // forward the network and the loss function on all training datapoints
    let loss = new Value(0);
    for (const [x, y] of split) {
        const logits = model.forward([new Value(x[0]), new Value(x[1])]);
        loss = loss.add(crossEntropy(logits, y));
    }
    loss = loss.mul(1.0 / split.length); // normalize the loss
    return loss;
}

function renderCanvas(minX = -2, minY = -2, maxX = 2, maxY = 2) {
    // first render the datapoints
    const canvas = document.getElementById('decision-canvas');
    const ctx = canvas.getContext('2d');

    // Clear the canvas
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // Function to map data points to canvas coordinates
    function mapToCanvas(x, y) {
        const canvasX = (x - minX) * (canvas.width / (maxX - minX));
        const canvasY = (maxY - y) * (canvas.height / (maxY - minY));
        return [canvasX, canvasY];
    }

    // Render the current decision surface
    const stepSize = 0.1;
    const rectWidth = stepSize * canvas.width / (maxX - minX);
    const rectHeight = stepSize * canvas.height / (maxY - minY);
    for (let x = minX; x < maxX; x += stepSize) {
        for (let y = minY; y < maxY; y += stepSize) {
            const centerX = x + stepSize / 2;
            const centerY = y + stepSize / 2;
            const logits = model.forward([new Value(centerX), new Value(centerY)]);
            const exps = logits.map(logit => Math.exp(logit.data));
            const sumExps = exps.reduce((a, b) => a + b, 0);
            const probs = exps.map(exp => exp / sumExps);

            const r = Math.floor(probs[0] * 255);
            const g = Math.floor(probs[1] * 255);
            const b = Math.floor(probs[2] * 255);

            const [canvasX, canvasY] = mapToCanvas(x, y);
            const [canvasX2, canvasY2] = mapToCanvas(x + stepSize, y + stepSize);
            const width = canvasX2 - canvasX;
            const height = canvasY - canvasY2;
            const mutedR = Math.floor(r + (255 - r) * 0.5);
            const mutedG = Math.floor(g + (255 - g) * 0.5);
            const mutedB = Math.floor(b + (255 - b) * 0.5);
            ctx.fillStyle = `rgb(${mutedR},${mutedG},${mutedB})`;
            ctx.strokeStyle = `rgb(${mutedR},${mutedG},${mutedB})`;
            ctx.fillRect(canvasX, canvasY2, width, height);
            ctx.strokeRect(canvasX, canvasY2, width, height);
        }
    }

    // Render training data points
    for (const [x, y] of trainSplit) {
        const [canvasX, canvasY] = mapToCanvas(x[0], x[1]);
        ctx.fillStyle = y === 0 ? 'red' : y === 1 ? 'green' : 'blue';
        ctx.beginPath();
        ctx.arc(canvasX, canvasY, 5, 0, 2 * Math.PI);
        ctx.fill();
        ctx.strokeStyle = 'black';
        ctx.stroke();
    }

    // Render the 0-level set of all individual neurons
    for (const neuron of model.layers[0].neurons) {
        const w0 = neuron.w[0].data;
        const w1 = neuron.w[1].data;
        const b = neuron.b.data;
        const x1 = -2;
        const y1 = (-b - w0 * x1) / w1;
        const x2 = 2;
        const y2 = (-b - w0 * x2) / w1;
        const [canvasX1, canvasY1] = mapToCanvas(x1, y1);
        const [canvasX2, canvasY2] = mapToCanvas(x2, y2);
        ctx.strokeStyle = 'white';
        ctx.beginPath();
        ctx.moveTo(canvasX1, canvasY1);
        ctx.lineTo(canvasX2, canvasY2);
        ctx.stroke();
    }
}

function renderOptimizerState(num_columns = 3) {
    const optimizerDiv = document.getElementById('optimizer-div');
    optimizerDiv.innerHTML = '';

    const parameters = model.parameters();
    const rows = Math.ceil(parameters.length / num_columns);

    for (let col = 0; col < num_columns; col++) {
        const table = document.createElement('table');
        table.innerHTML = '<tr><th>param</th><th>-m/sqrt(v)</th><th>grad</th><th>m</th><th>sqrt(v)</th></tr>';
        table.style.display = 'inline-block';
        table.style.marginRight = '10px';
        table.style.verticalAlign = 'top';

        for (let row = 0; row < rows; row++) {
            const index = col * rows + row;
            if (index >= parameters.length) break;

            const param = parameters[index];
            const sqrtV = Math.sqrt(param.v + 1e-8);
            const update = -param.m / sqrtV; // the -sign is because we update params with -=
            const tableRow = document.createElement('tr');
            tableRow.innerHTML = `
                <td style="color: ${param.data >= 0 ? '#45a049' : '#e06666'}">${param.data.toFixed(4)}</td>
                <td style="color: ${update >= 0 ? '#45a049' : '#e06666'}">${update.toFixed(4)}</td>
                <td style="color: ${param.grad >= 0 ? '#45a049' : '#e06666'}">${param.grad.toFixed(4)}</td>
                <td style="color: ${param.m >= 0 ? '#45a049' : '#e06666'}">${param.m.toFixed(4)}</td>
                <td style="color: ${sqrtV >= 0 ? '#45a049' : '#e06666'}">${sqrtV.toFixed(4)}</td>
            `;
            table.appendChild(tableRow);
        }

        optimizerDiv.appendChild(table);
    }
}

let step = 0;
function trainAndRenderStep() {
    if (step < 100) {
        // evaluate the validation split every few steps
        if (step % 10 === 0) {
            const valLoss = lossFun(model, valSplit);
            console.log(`step ${step+1}, val loss ${valLoss.data.toFixed(6)}`);
        }
        // get the loss for the training split
        const trainLoss = lossFun(model, trainSplit);
        console.log(`step ${step+1}, train loss ${trainLoss.data.toFixed(6)}`);

        // backward pass (deposit the gradients)
        trainLoss.backward();

        // render the current state
        renderCanvas();
        renderOptimizerState();

        // update with AdamW
        optimizer.step();
        optimizer.zeroGrad();

        step++;
        setTimeout(trainAndRenderStep, 100);
    }
}
trainAndRenderStep();

</script>
</body>
</html>